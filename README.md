# Seq2Seq Translation with LSTM in PyTorch

Этот проект реализует простую модель перевода на основе архитектуры sequence-to-sequence (seq2seq) с использованием LSTM. Модель обучается на парных предложениях на английском и русском языках. Проект включает этапы предобработки данных, построения словарей, разбиения выборки, создания кастомного датасета и загрузчиков, а также реализацию модели, функций обучения и валидации.

## Содержание

- [Обзор проекта](#обзор-проекта)
- [Особенности](#особенности)
- [Установка](#установка)
- [Датасет](#датасет)
- [Использование](#использование)
- [Архитектура модели](#архитектура-модели)
- [Обучение](#обучение)
- [Результаты](#результаты)
- [Метрики](#метрики)
- [Зависимости](#зависимости)


## Обзор проекта

В этом проекте реализована базовая модель перевода с помощью seq2seq архитектуры:
- **Предобработка данных:** Очистка текста от лишних символов, приведение к нижнему регистру, удаление чисел и лишних пробелов.
- **Построение словарей:** Создание отображения токенов в числовые идентификаторы для английского и русского языков.
- **Разбиение выборки:** Деление данных на обучающую, валидационную и тестовую выборки.
- **Кастомный датасет и загрузчик:** Использование `torch.utils.data.Dataset` и `DataLoader` с кастомной функцией `collate_fn` для обработки переменной длины последовательностей.
- **Модель seq2seq:** Реализация энкодера и декодера с использованием LSTM, а также использование teacher forcing во время обучения.
- **Обучение и валидация:** Функции для обучения модели с градиентным клиппингом и для оценки модели на валидационной выборке.
- **Визуализация:** Построение графика изменения функции потерь (loss) по эпохам с помощью Matplotlib.

## Особенности

- **Предобработка текста:** Очистка, нормализация и токенизация предложений.
- **Гибкая архитектура:** Возможность настройки числа слоев, размера скрытого состояния, dropout и других гиперпараметров.
- **Teacher Forcing:** Использование стратегии обучения для улучшения сходимости.
- **Визуализация:** График обучения помогает отслеживать прогресс модели.

## Установка

1. **Клонируйте репозиторий:**

    ```bash
    git clone https://github.com/yourusername/seq2seq-translation.git
    cd seq2seq-translation
    ```

2. **Установите необходимые зависимости:**

    ```bash
    pip install -r requirements.txt
    ```

## Датасет

Проект использует датасет с Kaggle — файл `rus.txt`, содержащий пары предложений на английском и русском языках. Файл должен находиться по пути:
/kaggle/input/lang-data/rus.txt

Если вы запускаете проект вне среды Kaggle, измените путь к файлу в строке загрузки датасета:

```python
data = pd.read_csv("path/to/your/rus.txt", delimiter='\t', header=None)
```

Файл предполагается табличным, где:

Первая колонка — английский текст.
Вторая колонка — русский текст.
Третья колонка (если присутствует) может содержать комментарии, но в проекте используется только первые две.

## Использование

Запустите основной скрипт main_code.ipynb, который включает все этапы от предобработки до обучения модели:
В процессе выполнения:

- Данные будут очищены и токенизированы.
- Построены словари для обоих языков.
- Датасет будет разделён на обучающую, валидационную и тестовую выборки.
- Будет создана и обучена seq2seq модель.
- Будут выведены значения функции потерь на обучающей и валидационной выборках для каждой эпохи.
- По завершении обучения будет построен график изменения потерь.

## Архитектура модели

**Энкодер**

- **Embedding Layer:** Преобразует входные токены в векторное представление.
- **LSTM:** Обрабатывает последовательность векторов и выдаёт скрытые состояния.
- **Dropout:** Применяется для регуляризации.
  
**Декодер**

- **Embedding Layer:** Преобразует входной токен (или предыдущий прогноз) в вектор.
- **LSTM:** Генерирует выходной вектор на каждом временном шаге.
- **Linear Layer:** Преобразует выходной вектор в вероятностное распределение по токенам словаря.
  
**Seq2Seq**

- **Комбинация энкодера и декодера:** Передача скрытых состояний энкодера в декодер.
- **Teacher Forcing:** Использование целевых токенов с вероятностью 0.5 во время обучения для улучшения сходимости.

## Обучение
**Конфигурация обучения:**

- **Оптимизатор:** AdamW с lr=0.001.
- **Функция потерь:** CrossEntropyLoss (игнорируется индекс паддинга).
- **Teacher Forcing Ratio:** 0.5 (настраивается в функции forward модели).
- **Клиппинг градиентов:** Применяется с порогом CLIP=1.0.
- **Количество эпох:** 10
- **Размер батча:** 32

**В каждой эпохе происходит:**

**1.** Прямой проход через модель.

**2.** Вычисление функции потерь (исключая начальный токен <SOS>).

**3.** Обратное распространение ошибки и обновление параметров модели.

**4.** Вычисление и вывод значений потерь на обучающей и валидационной выборках.

## Результаты
После каждой эпохи выводятся значения функции потерь для обучающей и валидационной выборок. В конце обучения с помощью Matplotlib строится график, демонстрирующий динамику обучения.

## Метрики

Для понимания корректности работы модели использовалась BLEU метрика. После 10 эпох обучения **BLEU = 0.5225**

## Зависимости
Полный список зависимостей указан в файле requirements.txt.
